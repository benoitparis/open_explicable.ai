{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Global interpretation - Separation space\n",
    "\n",
    "We project the leaf node participations into a 3D space, and package representations to visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cleaning import Cleaning \n",
    "df, labelMapping = Cleaning.from_path(\"AmesHousing.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,:-1], df.iloc[:, -1] # Last one by convention\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rulefitcustom import RuleFitCustom\n",
    "from sklearn import manifold\n",
    "\n",
    "features = X.columns\n",
    "X_mat = X.values\n",
    "\n",
    "# max_rules empricirally chosen so as to maximize the graphical representation's AUC (at the end of this notebook)\n",
    "# 400 c'est beau\n",
    "rf = RuleFitCustom(max_rules=400, model_type='r')\n",
    "rf.fit(X_mat, y, feature_names=features)\n",
    "prediction = rf.predict(X_mat)\n",
    "prediction_df = pd.DataFrame({'__prediction' : prediction})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Shap values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf.tree_generator)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap_values_df = pd.DataFrame(shap_values, columns=list(map(lambda x: \"attribution_\" + x, features.values)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf.tree_generator)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap_values_df = pd.DataFrame(shap_values, columns=list(map(lambda x: \"attribution_\" + x, features.values)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# par opposition à rf.X_rules, qui est weighed. Bon ici on weigh pas. Mais si on veut mix les deux il faudra faire attention\n",
    "binaryParticipations = rf.rule_ensemble.transform(X_mat, weigh_rules = False)\n",
    "# ou bien on la fait robuste\n",
    "binaryParticipations = (binaryParticipations > 0.0000001)\n",
    "\n",
    "np.savetxt(\"binary-participations.csv\", binaryParticipations, delimiter=\",\", fmt=\"%i\")\n",
    "dfBinaryParticipations = pd.DataFrame(binaryParticipations)\n",
    "dfBinaryParticipations.rename(columns=lambda x: \"{}\".format(x), inplace=True)\n",
    "dfBinaryParticipations.to_parquet('binary-participations.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# par opposition à rf.X_rules, qui est weighed. Bon ici on weigh pas. Mais si on veut mix les deux il faudra faire attention\n",
    "binaryParticipations = rf.rule_ensemble.transform(X_mat, weigh_rules = False)\n",
    "# ou bien on la fait robuste\n",
    "binaryParticipations = (binaryParticipations > 0.0000001)\n",
    "\n",
    "np.savetxt(\"binary-participations.csv\", binaryParticipations, delimiter=\",\", fmt=\"%i\")\n",
    "dfBinaryParticipations = pd.DataFrame(binaryParticipations)\n",
    "dfBinaryParticipations.rename(columns=lambda x: \"{}\".format(x), inplace=True)\n",
    "dfBinaryParticipations.to_parquet('binary-participations.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# par opposition à rf.X_rules, qui est weighed. Bon ici on weigh pas. Mais si on veut mix les deux il faudra faire attention\n",
    "binaryParticipations = rf.rule_ensemble.transform(X_mat, weigh_rules = False)\n",
    "# ou bien on la fait robuste\n",
    "binaryParticipations = (binaryParticipations > 0.0000001)\n",
    "\n",
    "np.savetxt(\"binary-participations.csv\", binaryParticipations, delimiter=\",\", fmt=\"%i\")\n",
    "dfBinaryParticipations = pd.DataFrame(binaryParticipations)\n",
    "dfBinaryParticipations.rename(columns=lambda x: \"{}\".format(x), inplace=True)\n",
    "dfBinaryParticipations.to_parquet('binary-participations.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# par opposition à rf.X_rules, qui est weighed. Bon ici on weigh pas. Mais si on veut mix les deux il faudra faire attention\n",
    "binaryParticipations = rf.rule_ensemble.transform(X_mat, weigh_rules = False)\n",
    "# ou bien on la fait robuste\n",
    "binaryParticipations = (binaryParticipations > 0.0000001)\n",
    "\n",
    "np.savetxt(\"binary-participations.csv\", binaryParticipations, delimiter=\",\", fmt=\"%i\")\n",
    "dfBinaryParticipations = pd.DataFrame(binaryParticipations)\n",
    "dfBinaryParticipations.rename(columns=lambda x: \"{}\".format(x), inplace=True)\n",
    "dfBinaryParticipations.to_parquet('binary-participations.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conf = {}\n",
    "conf['features'] = features.to_list()\n",
    "conf['predicted_variables'] = [y.name]\n",
    "conf['mean'] = y.mean()\n",
    "conf['std'] = y.std()\n",
    "conf['label_mapping'] = labelMapping\n",
    "conf['datapoint_number'] = df.shape[0]\n",
    "conf['rule_number'] = len(rf.rule_ensemble.rules)\n",
    "conf['binary-participations'] = 'binary-participations.csv'\n",
    "conf['rule-definitions'] = 'rule-definitions.csv'\n",
    "conf['data-prediction-embedding-cluster'] = 'data-prediction-embedding-cluster.csv'\n",
    "\n",
    "with open('conf.json', 'w') as outfile:\n",
    "    json.dump(conf, outfile, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# par opposition à rf.X_rules, qui est weighed. Bon ici on weigh pas. Mais si on veut mix les deux il faudra faire attention\n",
    "binaryParticipations = rf.rule_ensemble.transform(X_mat, weigh_rules = False)\n",
    "# ou bien on la fait robuste\n",
    "binaryParticipations = (binaryParticipations > 0.0000001)\n",
    "\n",
    "np.savetxt(\"binary-participations.csv\", binaryParticipations, delimiter=\",\", fmt=\"%i\")\n",
    "dfBinaryParticipations = pd.DataFrame(binaryParticipations)\n",
    "dfBinaryParticipations.rename(columns=lambda x: \"{}\".format(x), inplace=True)\n",
    "dfBinaryParticipations.to_parquet('binary-participations.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Rule' : rf.rule_ensemble.rules}).to_csv('rule-definitions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PROGRAM FILES\\ANACONDA3\\lib\\site-packages\\umap\\spectral.py:229: UserWarning: Embedding a total of 4 separate connected components using meta-embedding (experimental)\n",
      "  n_components\n"
     ]
    }
   ],
   "source": [
    "# UMAP\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# https://stats.stackexchange.com/questions/235882/pca-in-numpy-and-sklearn-produces-different-results\n",
    "x_std = StandardScaler().fit_transform(rf.X_rules)\n",
    "embedding_data = PCA(n_components=50).fit_transform(x_std)\n",
    "embedding = umap.UMAP(n_components=3).fit_transform(embedding_data)\n",
    "embedding = embedding - embedding.mean(axis=0) # faudra care au predict a partir du umap à mettre ça aussi\n",
    "embedding_df = pd.DataFrame({'x' : embedding[:,0], 'y' : embedding[:,1], 'z' : embedding[:,2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Cluster structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "clusterer.fit(embedding_df)\n",
    "tree_ct_df = clusterer.condensed_tree_.to_pandas().join(embedding_df, on='child')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add root if not present\n",
    "if (tree_ct_df.loc[tree_ct_df['child'] == df.shape[0]].shape[0] == 0):\n",
    "    root = pd.DataFrame([[float('NaN'), df.shape[0], float('NaN'), df.shape[0], float('NaN'), float('NaN'), float('NaN')]], columns=tree_ct_df.columns)\n",
    "    tree_ct_df = tree_ct_df.append(root)\n",
    "    tree_ct_df = tree_ct_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gb = tree_ct_df.groupby(tree_ct_df['parent'])\n",
    "\n",
    "def getAndBuildCentroidValues(index):\n",
    "    localDf = tree_ct_df.loc[gb.groups[index]]\n",
    "    total = (0, 0, 0)\n",
    "    weight = 0 \n",
    "    for localIndex, row in localDf.iterrows():\n",
    "        localTuple = row[['x', 'y', 'z']]\n",
    "        if (np.isnan(localTuple[0])):\n",
    "            localTuple = getAndBuildCentroidValues(row['child'])\n",
    "            tree_ct_df.iloc[localIndex, 5] = localTuple[0]\n",
    "            tree_ct_df.iloc[localIndex, 6] = localTuple[1]\n",
    "            tree_ct_df.iloc[localIndex, 7] = localTuple[2]\n",
    "        total = (total[0] + localTuple[0] * row['child_size'], total[1] + localTuple[1] * row['child_size'], total[2] + localTuple[2] * row['child_size'])\n",
    "        weight = weight + row['child_size']\n",
    "    result = (total[0] / weight, total[1] / weight, total[2] / weight)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rootTuple = getAndBuildCentroidValues(df.shape[0])\n",
    "\n",
    "tree_ct_df.iloc[-1, 5] = rootTuple[0]\n",
    "tree_ct_df.iloc[-1, 6] = rootTuple[1]\n",
    "tree_ct_df.iloc[-1, 7] = rootTuple[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Export instance-level info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# care format\n",
    "export_tree_df = tree_ct_df[['child', 'parent', 'lambda_val', 'child_size', 'x', 'y', 'z']].set_index('child')\n",
    "output = df.join(prediction_df).join(export_tree_df, how='outer').join(shap_values_df).reset_index().sort_values(by=['index'])\n",
    "output.to_csv('data-prediction-embedding-cluster.csv', index=False)\n",
    "output.to_parquet('data-prediction-embedding-cluster.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluation of this 3D space as a map of the different behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Faudra évaluer, et comparer au RF plus haut?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (python3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}